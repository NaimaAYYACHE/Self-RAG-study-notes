**Paper:** *Selfâ€‘RAG: Learning to Retrieve, Generate, and Critique through Selfâ€‘Reflection* (Akari Asai et al., arXiv 2310.11511)


<img width="461" height="443" alt="Capture dâ€™Ã©cran 2025-11-05 130453" src="https://github.com/user-attachments/assets/8f728b50-4fbd-40a4-bdd1-21589d2497c5" />



you can see the pdf here : [Self RAG.pdf](https://github.com/user-attachments/files/23398431/Self.RAG.pdf)

---


# **ğŸŒ Part 1 â€” Background: Classic RAG**

## **1ï¸âƒ£ What is RAG?**

**RAG (Retrieval-Augmented Generation)** is a framework that combines:

- **Retrieval models** ğŸ” â†’ fetch external information (from a database or knowledge source)
- **Generator models (LLMs)** âœï¸ â†’ produce final answers using both retrieved and internal knowledge.

ğŸ’¡ **Goal:**

Make LLMs more factual by grounding their answers in external documents instead of relying only on what they â€œrememberâ€ in their parameters (weights ).

---

## **2ï¸âƒ£ How RAG Works**
![standard RAG](https://github.com/user-attachments/assets/0a4a54ea-6641-4238-ac80-d4183835d7ef)
---

## **3ï¸âƒ£ RAGâ€™s Limitations ğŸš«**

| Problem | Description | Example |
| --- | --- | --- |
| **Fixed Retrieval** | Always retrieves a fixed number (e.g., top-5) docs â€” even if not needed. | Unnecessary retrieval wastes computation ğŸ’¸ |
| **Irrelevant Documents** | Retrieved docs may not match the query intent. | Retrieval noise â†’ confusing output ğŸ˜µ |
| **No Self-Evaluation** | Model canâ€™t judge if info is relevant or factual. | Can repeat hallucinations confidently ğŸ˜¬ |
| **Static Pipeline** | Retrieval and generation are separate modules. | Lacks interaction and reflection ğŸ”’ |

These challenges inspired the creation of **SELF-RAG**.

---

# **ğŸš€ Part 2 â€” Enter SELF-RAG**

**SELF-RAG (Self-Reflective Retrieval-Augmented Generation)** is an advanced version of **RAG** that makes the model **think, check, and correct itself** ğŸ¤–ğŸª.

It combines:

- ğŸ§  **Reasoning (like an LLM)** â€” to generate fluent text.
- ğŸ” **Retrieval (like RAG)** â€” to get real facts from external sources.
- ğŸ’­ **Self-Reflection** â€” to **evaluate its own answers** and improve factual accuracy.

Its a model that can:

- **Decide when** to retrieve ğŸ”
- **Evaluate** the **relevance** and **support** of retrieved docs ğŸ§ 
- **Critique** its own outputs âœï¸
- **Adapt** generation dynamically

In short:

ğŸ§© *It retrieves when necessary, reflects during generation, and critiques afterward.*

---

# ğŸ§  Part â€” 3 **Adaptive Retrieval**

Self-RAG makes retrieval **dynamic** â€” the model itself **decides when to retrieve and how much to retrieve**, based on its confidence and reasoning.

During generation, the model emits a *special internal signal* (called a **retrieval token**) like:

- `[RETRIEVE]` â†’ *â€œI donâ€™t know enough, fetch supporting docs.â€*
- `[NO_RETRIEVE]` â†’ *â€œIâ€™m confident, I can answer from memory.â€*

Once retrieval happens, the model integrates the new passages and continues generating the answer.

So instead of **fixed retrieval**, you get **on-demand retrieval.**

---

# **âš™ï¸ Part 4 â€” Architecture Overview**

## 1ï¸âƒ£ Two Models

- **Generator Model (M)** âœï¸ğŸ¤– â†’ creates text and emits *reflection tokens* during generation
- **Critic Model (C)** ğŸ‘©â€ğŸ« â†’ evaluates and labels data with reflection tokens during training

> Together, they learn to retrieve, generate, and critique without external supervision.
> 
<img width="8948" height="4734" alt="teaser_self_rag_v8" src="https://github.com/user-attachments/assets/50ebebe8-fd3c-4b4c-abec-259a47b77de1" />


---

## **2ï¸âƒ£ Generator Model (M) â€” â€œThe Writer & Self-Reflectorâ€ âœï¸ğŸ¤–ğŸ§ **

### **1. Role**

- Its main job is to **generate text** in response to a user query ğŸ“ğŸ’¬.
- While generating, it **predicts reflection tokens** `([RETRIEVE] ğŸ”, [ISREL] âœ…, [ISSUP] ğŸ“–, [ISUSE] â­`) to guide its own reasoning ğŸ§ .
- It decides **when to retrieve information** ğŸ“š and whether the generated output is supported by evidence ğŸ“Š.
- Essentially, the Generator is the **decision maker & writer** âœï¸ğŸ§ , producing answers and self-critiquing them for quality and factuality ğŸ†.

### **2. When it works**

- **During inference** ğŸ”„ (i.e., when answering a user query).
    - **For every new user query**, the Generator:
        1. Decides if it needs to retrieve documents `([RETRIEVE]).`
        2. Evaluates relevance `([ISREL])` and support `([ISSUP]`) of retrieved info.
        3. Generates the answer while self-reflecting using these tokens `([ISUSE]).`
- **During training** ğŸ«, it learns to predict both **text** and **reflection tokens** from data annotated by the Critic model  ğŸ‘©â€ğŸ«.
    - The Generator **applies the Criticâ€™s lessons in real time** but without the ~~Critic mode~~l being involved.
<img width="1139" height="267" alt="image" src="https://github.com/user-attachments/assets/ae9dbf59-00d3-4cbe-8c44-6487c8998cf8" />

---
### **3. Step-by-Step Generator Process**

**1ï¸âƒ£ Generator decides retrieval ğŸ”**

- The Generator evaluates its own knowledge when it starts generating.
    - Confident â†’ skip retrieval â†’ `[RETRIEVE=NO]` âœ…
    - Uncertain â†’ trigger retrieval â†’ `[RETRIEVE=YES]` ğŸ”
    - Reuse previous evidence â†’ `[RETRIEVE=CONTINUE]` ğŸ”„

**2ï¸âƒ£ Generator evaluates relevance of documents ğŸ“„**

- After retrieval, it reads retrieved passages.
- Judges each passageâ€™s relevance internally and assigns reflection tokens:
    - `[ISREL=1]` â†’ relevant âœ…
    - `[ISREL=0]` â†’ irrelevant âŒ
- Focuses only on **useful evidence** for the answer.


**3ï¸âƒ£ Generator produces reflection tokens alongside output ğŸ§ **

- Generates text **and reflection tokens together**:
    - `[ISREL]` â†’ relevance of retrieved doc
    - `[ISSUP]` â†’ whether generated statement is supported ğŸ“–
    - `[ISUSE]` â†’ usefulness of the sentence â­
- These tokens **guide reasoning step by step** and allow **self-reflection**.

**4ï¸âƒ£ Generator improves final output âœ¨**

- Using reflection tokens, it can:
    - **Refine answers** if evidence is weak ğŸ”„
    - **Select the best candidate** from multiple segments âœ…
    - **Balance fluency vs factuality** depending on the task ğŸ›ï¸

---

### **4. Example (Morocco 2022 âš½ğŸ‡²ğŸ‡¦)**

> User: â€œHow did Morocco perform in the 2022 World Cup?â€ âš½ğŸ‡²ğŸ‡¦
> 

Generatorâ€™s thinking process:

```
Hmmâ€¦ unsure ğŸ¤” â†’ [RETRIEVE] ğŸ”
(retrieves FIFA 2022 documents ğŸ“„ğŸ“‘)
Sentence: â€œMorocco became the first African and Arab team to reach the semifinals.â€
[ISREL=1] âœ… [ISSUP=1] ğŸ“– [ISUSE=5] â­
```

- The generator wrote the sentence **and** assigned reflection tokens ğŸ“ğŸ§ .
- These tokens **indicate what the model thinks about relevance, support, and usefulness** ğŸ”ğŸ“Š.
- This annotated output helps **improve generation quality** and **control inference behavior** ğŸ¯

---

## **3ï¸âƒ£** **Critic Model (C) â€” â€œThe Teacher & Annotatorâ€ ğŸ‘©â€ğŸ«ğŸ“š**

### **1. Role**

- Its main job is **not to generate text** âŒğŸ“.
- Instead, it **evaluates outputs** (generated by the Generator or from a dataset) and **labels them with reflection tokens** ğŸ·ï¸.
- Essentially, it **provides â€œground truthâ€ reflection tokens** that the Generator can learn from ğŸ“–ğŸ§ .
<img width="805" height="353" alt="image" src="https://github.com/user-attachments/assets/5ca18279-ad46-44de-8f27-dcb86b040387" />


### **2. When it works - Training Phase Only ğŸ‘©â€ğŸ«**

- The Critic model is used **during the training of the Generator** ğŸ«ğŸ“š.
- It **labels the training data** with reflection tokens:
    - Does the model need to retrieve external information? â†’ `[RETRIEVE] ğŸ”`
    - Are retrieved docs relevant? â†’ `[ISREL] âœ…`
    - Is the output supported by the documents? â†’ `[ISSUP] ğŸ“–`
    - Is the output useful to answer the query? â†’ `[ISUSE] â­`
- This teaches the Generator **how to self-reflect** when generating answers.
- âœ… After this training is done, the Criticâ€™s job is essentially complete. Itâ€™s **offline**.
  
---
### **3. Example (Morocco 2022 âš½ğŸ‡²ğŸ‡¦)**

Generator output:

> â€œMorocco became the first African and Arab team to reach the World Cup semifinals.â€
> 

Critic evaluates:

```
âœ… Relevant: doc discusses Morocco 2022 semifinals â†’ [ISREL=1] âœ…
âœ… Supported: statement matches doc â†’ [ISSUP=1] ğŸ“–
â­ Useful doc â†’ [ISUSE=5] â­
```

It annotates the training data like:

```
[RETRIEVE] Morocco became the first African and Arab team to reach 
the World Cup semifinals. [ISREL=1] âœ… [ISSUP=1] ğŸ“– [ISUSE=5] â­
```

- Generator then learns:

> â€œWhen I write similar sentences in the future, I should mark them as supported and relevant.â€ ğŸ§ ğŸ’¡
> 

---

## **4ï¸âƒ£**How Critic and Generator Interact

### **Critic Model (C) â€” the Teacher ğŸ‘©â€ğŸ«ğŸ“š**

- ğŸ‘€ Looks at the **retrieved documents** ğŸ“„ğŸ“„ and the **generatorâ€™s output** âœï¸ğŸ¤–.
- ğŸ·ï¸ Assigns **reflection tokens** `([RETRIEVE] ğŸ”, [ISREL] âœ…, [ISSUP] ğŸ“–, [ISUSE] â­)` to the training data.
- ğŸ“ These tokens **label :  which passages are relevant ?**, **which statements are supported ?** , and **how useful the answer is ?**  ğŸ“Š.
- âŒ Does **not generate answers** â€” it only **teaches the generator where and how to reflect** ğŸ§ .

### **Generator Model (M) â€” the Student âœï¸ğŸ¤–ğŸ§ **

- ğŸ“– Learns from the **Critic-labeled data** where reflection tokens should appear ? .
- ğŸ”„ During **inference**, it **generates answers** and **self-reflects** using these tokens.
- ğŸ•µï¸â€â™‚ï¸ Decides **when to retrieve ?** , **which docs are relevant ?** ğŸ“„, and **how well the answer is supported ?** ğŸ“Š.
- âœ¨ Can **refine its output** using reflection tokens, essentially doing **self-critique** ğŸ§.

## **5ï¸âƒ£Generator vs Critic â€“ Quick Comparison Table**

| Feature | Generator (M) âœï¸ğŸ¤– | Critic (C) ğŸ‘©â€ğŸ«ğŸ“š |
| --- | --- | --- |
| **Main job** | Generate text + predict reflection tokens ğŸ“ğŸ§  | Evaluate outputs + assign reflection tokens ğŸ·ï¸ğŸ“Š |
| **Active when** | Training & inference ğŸ”„ | Only during training ğŸ« |
| **User-facing** | Yes ğŸ’¬ | No âŒ |
| **Purpose** | Self-reflection & adaptive retrieval ğŸ”ğŸ§  | Provide ground truth labels for training ğŸ“– |
| **Example output** | Text + `[ISREL] [ISSUP] [ISUSE]` ğŸ“âœ…ğŸ“–â­ | Only `[ISREL] [ISSUP] [ISUSE]` labels ğŸ·ï¸âœ…ğŸ“–â­ |
| **Key actions** | Decide retrieval ğŸ•µï¸â€â™‚ï¸, judge relevance ğŸ“„, refine output ğŸ† | Label relevance âœ…, support ğŸ“–, usefulness â­ |


###  âœ… Summary

- **Critic:** Offline teacher during training only ğŸ“šğŸ‘©â€ğŸ«
- **Generator:** Active student during training and real-time thinker during inference âœï¸ğŸ¤–ğŸ§ 

---

# ğŸ“š Part 5 â€” **Reflection Tokens in SELF-RAG**

**Reflection tokens** are **special markers** that help the model **analyze and monitor its own reasoning process** during generation.

They tell the model *how **confident, supported**, or **relevant*** its statements are.

## 1ï¸âƒ£ ğŸ§© Main Reflection Tokens
| Token | Meaning | Purpose |
| --- | --- | --- |
| `[RETRIEVE]` | Indicates if the model should fetch external information ğŸ” | Ensures answers are factually supported by relevant documents âœ… |
| `[ISREL]` | Is the **retrieved document relevant** ? ğŸ“š | Judges if the **retrieved doc** actually helps **answer the question.** |
| `[ISSUP]` | Is the statement **supported** by evidence ***( retrieved doc)***? âœ… | Marks whether the **generated answer**  matches **retrieved docs.** |
| `[ISUSE]` | How **useful** the document is for answering â­ | Measures doc usefulness (e.g., 1â€“5 scale). |

<img width="861" height="392" alt="Capture dâ€™Ã©cran 2025-11-06 205703" src="https://github.com/user-attachments/assets/cfd0f1c1-81fc-49aa-bba1-162adc1ee913" />

ğŸ” see more about  [Reflection Tokens](notes/3.%20Reflection%20tokens.md) you will find  Explanation of `[RETRIEVE]`, `[ISREL]`, `[ISSUP]`, `[ISUSE]` tokens 

---


## **2ï¸âƒ£** âš½ **Example: Morocco in the 2022 World Cup**

> User: â€œWhat did Morocco achieve in the 2022 FIFA World Cup?â€
> 

**Generator thinking process:**

```
Hmmâ€¦ I might need real facts â†’ **[RETRIEVE]**
(retrieves sports news 2022)

Doc 1 = talks about Morocco reaching the semi-final **[ISREL=1]**

Doc 2 = talks about Argentina â†’ irrelevant **[ISREL=0]**

Sentence: â€œ **Morocco became the first African team to reach a World Cup semi-final.** â€ **[ISSUP=1]**

Doc usefulness = high **[ISUSE=5]**
```

So the model doesnâ€™t just say:

> â€œMorocco reached the 2022 World Cup semi-final.â€
> 

It actually generates something like:

> **[RETRIEVE]** Morocco reached the 2022 World Cup semi-final. **[ISREL=1] [ISSUP=1] [ISUSE=5]**
> 
---
### ğŸ’¡ **Why It Matters**

These tokens let the model:

- Track **how well-supported** each statement is ğŸ§ 
- Learn to **self-correct** low-confidence answers âš™ï¸
- Make generation more **factually grounded and transparent** ğŸ”

---

# **ğŸ“ Part 6 â€” How SELF-RAG Learns**

SELF-RAG training happens in **two main phases:**

## **1ï¸âƒ£ Offline Annotation (Critic Model C)**

- Uses a powerful LLM (like GPT-4) to insert reflection tokens into training data.
- The Critic model learns to predict those tokens automatically.

## **2ï¸âƒ£ Joint Training (Generator Model M)**

- The Generator learns both *what to write* and *when to reflect*.
- No need for reinforcement learning or reward models like RLHF â€” cheaper and more stable ğŸ’°.

---

# **âš–ï¸ Part 7 â€” Controllable Decoding**

At inference, **Self-RAG** predicts reflection tokens like `[REL=YES]` and `[SUP=YES]` with associated probabilities.

These probabilities can **ğŸ¯ steer generation** toward better, evidence-based answers.

- ğŸ¯ **Goal:** Prefer answers **supported by retrieved documents**, not just the most fluent ones.

- âš–ï¸ **Trade-off:** You can balance **factuality ğŸ§© vs fluency âœï¸** depending on your needs.

## ğŸ“Š **Example:**

| ğŸ—£ï¸ Candidate Answer | ğŸ’¬ Fluency Probability | ğŸ“š Support Probability (`SUP=YES`) |
| --- | --- | --- |
| â€œCasablanca is the capital of Morocco.â€ | 90 % | 20 % |
| â€œRabat is the capital of Morocco.â€ | 70 % | 95 % |
- ğŸ¤– **Standard LLM** picks the first (more fluent).
- ğŸ§  **Self-RAG** picks the second (better supported by evidence).

## âš–ï¸ **The Trade-Off: Fluency vs Factuality**

The paper explains you can â€œ**trade off fluency vs support**â€ â€” meaning you can **tune decoding** based on your goal:

- ğŸ§© **Prioritize factuality:** increase the weight of `[SUP=YES]` and `[REL=YES]`
    
    â†’ Model generates **evidence-backed answers** but might sound slightly less natural.
    
- âœï¸ **Prioritize fluency:** reduce their weight
    
    â†’ Model sounds **smoother**, but may be **less grounded** in facts.
    

---

# **ğŸ“ˆ Part 8 â€” Results & Impact**

They **trained and evaluated Self-RAG on both 7B and 13B models** 

However, they also **compared** those Self-RAG models to **non-RAG** and **standard RAG** baselines of the same or larger sizes.

So, in the experiments they had several setups like:

| Model Type | Size | Description |
| --- | --- | --- |
| Base LLM | 7B / 13B | Plain model without retrieval |
| Standard RAG | 7B / 13B | Model that always retrieves top-K documents |
| **Self-RAG** | 7B / 13B | Model that decides *when* to retrieve and *self-reflects* |

### âš™ï¸ **Their goal**

They wanted to test **two hypotheses**:

1. **Self-RAG vs. standard RAG (same size):**
    
    â†’ Does self-reflection improve factuality and efficiency compared to traditional RAG?
    
2. **Self-RAG vs. larger models:**
    
    â†’ Can a **Self-RAG 7B** outperform or match a **13B non-RAG or standard RAG** model?
    
    (i.e., can intelligence from *architecture* beat brute-force *scale*?)

### ğŸ“Š **Their findings**

- **Self-RAG (7B)** performed on par with or better than **standard RAG (13B)** on factual QA benchmarks like **Natural Questions** and **TriviaQA**.
- **Self-RAG (13B)** achieved the best overall factuality and citation support among all tested systems.

> They found that adding self-reflection (adaptive retrieval + critique tokens) improves factual reasoning more efficiently than simply increasing model size.
> 


### ğŸ§© **In short**

- They **used Self-RAG on both 7B and 13B models.**
- They **compared them** to non-RAG and standard RAG baselines.
- They discovered that **Self-RAGâ€™s reasoning gains beat size gains** â€” meaning thoughtful retrieval > just â€œbigger model.â€


**Insight:**

> A smaller SELF-RAG (7B) can outperform a much larger RAG (13B) because it learns *when* to retrieve and *how* to critique itself.
> 

| Benchmark | SELF-RAG 7B | Standard RAG 13B | ChatGPT | Result |
| --- | --- | --- | --- | --- |
| Factual QA | âœ… Higher | âŒ Lower | âš–ï¸ Close | Self-RAG wins ğŸ† |
| Citation Accuracy | âœ… Better | âŒ Lower | âš–ï¸ Close | Better grounding |
| Efficiency | âœ… Adaptive | âŒ Always retrieves | â€” | Less cost |
| Hallucination | â¬‡ï¸ Reduced | â¬†ï¸ Higher | âš–ï¸ Moderate | More reliable |

---

## ğŸ§ ğŸ¤– **Part 9 â€” SELF-RAG: End-to-End Process**

---

### **Step 1ï¸âƒ£ : User Query**

> User: â€œWhat was Moroccoâ€™s biggest achievement in the 2022 World Cup?â€ âš½ğŸ‡²ğŸ‡¦
> 

The system receives the query and the **Generator (M)** starts reasoning:

> ğŸ’­ â€œDo I have enough info?â€
> 
> 
> ğŸ¤” â€œHmm, thatâ€™s a factual eventâ€¦ I need details about the 2022 World Cup.â€
> 

### **Step 2ï¸âƒ£ : Generator decides whether to retrieve** `[RETRIEVE]`

- The generator checks its internal knowledge:
    - If confident â†’ `[RETRIEVE=NO]` âœ…
    - If unsure â†’ `[RETRIEVE=YES]` ğŸ” *(triggers retrieval)*
        
        **In this case:**
        
        ```
        [RETRIEVE=YES] ğŸ” (not stored in model memory)
        ```
        

### **Step 3ï¸âƒ£: Retriever fetches documents**

- The **Adaptive Retriever** searches the knowledge base.
- It returns multiple documents related to Moroccoâ€™s 2022 World Cup.

**Retrieved docs:**

ğŸ“„ **Doc A:** â€œMorocco became the first African and Arab team to reach the semifinals.â€

ğŸ“„ **Doc B:** â€œMoroccoâ€™s coach Walid Regragui took charge in August 2022.â€

ğŸ“„ **Doc C:** â€œArgentina won the 2022 World Cup.â€

### **Step 4ï¸âƒ£: Generator evaluates relevance** `[ISREL]`

- The generator reads all three documents and marks relevance:

```
Doc A â†’ [ISREL=1] âœ… (directly relevant to Moroccoâ€™s achievement)
Doc B â†’ [ISREL=1] âœ… (contextually relevant)
Doc C â†’ [ISREL=0] ğŸš« (irrelevant to Morocco)

```

âœ… Focuses only on **Doc A** and **Doc B** for generating the answer.


### **Step 5ï¸âƒ£: Generator writes outputs and predicts support** `[ISSUP]`

- The generator now creates factual sentences based on the relevant docs.

```
ğŸ† From Doc A â†’ â€œMorocco was the first African and Arab team to reach the World Cup semifinals.â€
ğŸ‘” From Doc B â†’ â€œWalid Regragui became Moroccoâ€™s coach in August 2022.â€
```

- It checks if both statements are **supported** by the documents:

```
Doc A sentence â†’ [ISSUP=1] âœ…
Doc B sentence â†’ [ISSUP=1] âœ…
```

### **Step 6ï¸âƒ£: Generator predicts usefulness** `[ISUSE]`

- The generator evaluates which answer best addresses the userâ€™s question:

```
ğŸ† Doc A â†’ [ISUSE=5] â­â­â­â­â­ â†’ extremely useful (main achievement)
ğŸ‘” Doc B â†’ [ISUSE=3] â­â­â­ â†’ moderately useful (background info)
```


### **Step 7ï¸âƒ£ : Optional second retrieval**

- If the model finds missing evidence, it may **retrieve again**:

```
[RETRIEVE=CONTINUE] ğŸ”„ â†’ fetch new docs if needed

```

- In this case, **Doc A** and **Doc B** are enough â€” no second retrieval needed.

### **Step 8ï¸âƒ£: Final output**

The model combines its decisions and produces the **final factual answer**:

> [RETRIEVE=YES] Morocco was the first African and Arab team to reach the World Cup semifinals. [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=5] â­â­â­â­â­
> 

âœ… The answer is **relevant**, **supported**, and **highly useful**.


### **Step 9ï¸âƒ£: Critic model role (training phase)**

- During training, the **Critic (C)** labels retrieved docs and model outputs with reflection tokens.
- The **Generator (M)** learns to self-assess its relevance, support, and usefulness at inference time.

### **Step ğŸ”Ÿ: Self-reflection & quality control**

- The generator can **reinspect** its reasoning path using reflection tokens:
    - Check relevance `[ISREL]`
    - Check support `[ISSUP]`
    - Check usefulness `[ISUSE]`

âœ… This ensures the **final answer is grounded, clear, and trustworthy** before returning to the user.

![What was Moroccoâ€™s biggest achievement in the 2022 World Cupâ€ ğŸ†ğŸ‡²ğŸ‡¦](https://github.com/user-attachments/assets/2f780ebf-c201-4dab-b4b0-9beafe31c1ad)


---

## **ğŸ’¡ Key Takeaways**

> âœ… **RAG** improves factual grounding by adding retrieval â€” but itâ€™s rigid.
> 

> âœ… **SELF-RAG** adds **self-awareness**: retrieves when needed, critiques its logic, and balances fluency vs factuality.
> 

> âœ… **Reflection tokens** turn reasoning into structured signals.
> 

> âœ… **Critic + Generator** training removes the need for complex reinforcement learning.
> 

> âœ… **End result:** Efficient, factual, transparent LLM reasoning.
> 

---
SELF-RAG combines **adaptive retrieval** + **self-reflection** to produce **more factual, efficient, and transparent outputs** than standard RAG or even larger LLMs.

---

### ğŸ“‘ Reports
| File | Description |
| --- | --- |
| [SELF-RAG Study Guide](reports/SELF%20RAG%20Study%20Guide.md) | Quiz, essay questions, and glossary for learning SELF-RAG |
| [Self RAG Report](reports/Self%20RAG%20report%20.md) | Full briefing and technical analysis from the research paper |


---

# ğŸ“ **Flashcards & Learning Tools**

To help you **memorize, practice, and test** the concepts, use these Quizlet links:

- ğŸ“‡ **Cards:**  
  - https://quizlet.com/1104301639/flashcards/embed  
  - https://quizlet.com/1104347417/flashcards/embed  

- ğŸ“š **Learn Mode:** [Practice & Learn](https://quizlet.com/1104347417/learn?funnelUUID=ab2047d9-3505-4ed4-af6f-9f2b84013ef3)  
- ğŸ“ **Test Yourself:** [Take a Test](https://quizlet.com/1104347417/test?answerTermSides=6&promptTermSides=6&questionCount=41&questionTypes=4&showImages=true)  
- ğŸ® **Play Games:**  
  - âš¡ [Blast](https://quizlet.com/blast/1104347417)  
  - ğŸ [Match](https://quizlet.com/1104347417/match?funnelUUID=684860c6-67ab-4aa1-b842-8fbde9549d62)  

ğŸ’¡ **Tip:** Review regularly, self-test, and make learning interactive & fun ğŸ‰  

---

ğŸ”— **References:**
- [Official Self-RAG GitHub](https://github.com/AkariAsai/self-rag?tab=readme-ov-file)  
- [Medium Summary](https://cobusgreyling.medium.com/self-reflective-retrieval-augmented-generation-self-rag-f5cbad4412d5)  
- [Notion Report](https://www.notion.so/SELF-RAG-Self-Reflective-Retrieval-Augmented-Generation-29f8343843d880a29b8ef54049665988?source=copy_link)
---

This repository is intended as a **learning and reference guide** for SELF-RAG based on the official research paper.
