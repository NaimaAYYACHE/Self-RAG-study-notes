# **âš–ï¸ SELF-RAG â€” Self-Reflective Retrieval-Augmented Generation â€” Controllable Decoding**

At inference, **Self-RAG** predicts reflection tokens like `[REL=YES]` and `[SUP=YES]` with associated probabilities.

These probabilities can **ğŸ¯ steer generation** toward better, evidence-based answers.

- ğŸ¯ **Goal:** Prefer answers **supported by retrieved documents**, not just the most fluent ones.
- âš–ï¸ **Trade-off:** You can balance **factuality ğŸ§© vs fluency âœï¸** depending on your needs.

---

## ğŸ“Š **Example:**

| ğŸ—£ï¸ Candidate Answer | ğŸ’¬ Fluency Probability | ğŸ“š Support Probability (`SUP=YES`) |
| --- | --- | --- |
| â€œCasablanca is the capital of Morocco.â€ | 90 % | 20 % |
| â€œRabat is the capital of Morocco.â€ | 70 % | 95 % |

- ğŸ¤– **Standard LLM** picks the first (more fluent).
- ğŸ§  **Self-RAG** picks the second (better supported by evidence).

---

## âš–ï¸ **The Trade-Off: Fluency vs Factuality**

The paper explains you can â€œ**trade off fluency vs support**â€ â€” meaning you can **tune decoding** based on your goal:

- ğŸ§© **Prioritize factuality:** increase the weight of `[SUP=YES]` and `[REL=YES]`
    
    â†’ Model generates **evidence-backed answers** but might sound slightly less natural.
    
- âœï¸ **Prioritize fluency:** reduce their weight
    
    â†’ Model sounds **smoother**, but may be **less grounded** in facts.
    

---

# **ğŸ“ˆ  Results & Impact**

They **trained and evaluated Self-RAG on both 7B and 13B models** 

However, they also **compared** those Self-RAG models to **non-RAG** and **standard RAG** baselines of the same or larger sizes.

So, in the experiments they had several setups like:

| Model Type | Size | Description |
| --- | --- | --- |
| Base LLM | 7B / 13B | Plain model without retrieval |
| Standard RAG | 7B / 13B | Model that always retrieves top-K documents |
| **Self-RAG** | 7B / 13B | Model that decides *when* to retrieve and *self-reflects* |

### âš™ï¸ **Their goal**

They wanted to test **two hypotheses**:

1. **Self-RAG vs. standard RAG (same size):**
    
    â†’ Does self-reflection improve factuality and efficiency compared to traditional RAG?
    
2. **Self-RAG vs. larger models:**
    
    â†’ Can a **Self-RAG 7B** outperform or match a **13B non-RAG or standard RAG** model?
    
    (i.e., can intelligence from *architecture* beat brute-force *scale*?)
    

---

### ğŸ“Š **Their findings**

- **Self-RAG (7B)** performed on par with or better than **standard RAG (13B)** on factual QA benchmarks like **Natural Questions** and **TriviaQA**.
- **Self-RAG (13B)** achieved the best overall factuality and citation support among all tested systems.

> They found that adding self-reflection (adaptive retrieval + critique tokens) improves factual reasoning more efficiently than simply increasing model size.
> 

---

### ğŸ§© **In short**

âœ… They **used Self-RAG on both 7B and 13B models.**

âœ… They **compared them** to non-RAG and standard RAG baselines.

âœ… They discovered that **Self-RAGâ€™s reasoning gains beat size gains** â€” meaning thoughtful retrieval > just â€œbigger model.â€

**Insight:**

A smaller SELF-RAG (7B) can outperform a much larger RAG (13B) because it learns *when* to retrieve and *how* to critique itself.

| Benchmark | SELF-RAG 7B | Standard RAG 13B | ChatGPT | Result |
| --- | --- | --- | --- | --- |
| Factual QA | âœ… Higher | âŒ Lower | âš–ï¸ Close | Self-RAG wins ğŸ† |
| Citation Accuracy | âœ… Better | âŒ Lower | âš–ï¸ Close | Better grounding |
| Efficiency | âœ… Adaptive | âŒ Always retrieves | â€” | Less cost |
| Hallucination | â¬‡ï¸ Reduced | â¬†ï¸ Higher | âš–ï¸ Moderate | More reliable |
