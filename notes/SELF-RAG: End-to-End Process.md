## **SELF-RAG: End-to-End Process** ğŸ§ ğŸ¤–

### **Step 1: User Query**

> User: â€œHow did Morocco perform in the 2022 World Cup?â€ âš½ğŸ‡²ğŸ‡¦
> 

The system receives the query and the generator (M) starts reasoning.

---

### **Step 2: Generator decides whether to retrieve** `[RETRIEVE]`

- The generator checks its parametric memory:
    - If confident: `[RETRIEVE=NO]` âœ…
    - If unsure: `[RETRIEVE=YES]` ğŸ” (triggers retrieval)

**In this case:**

```
[RETRIEVE=YES] ğŸ” (not in training data)

```
---

### **Step 3: Retriever fetches documents**

- External retrieval system searches the knowledge base.
- Returns multiple documents relevant to 2022 Morocco World Cup.

**Retrieved docs:**

1ï¸âƒ£ Doc A: â€œMorocco reached the semifinals after defeating Portugal.â€

2ï¸âƒ£ Doc B: â€œFrance won the 2018 World Cup.â€

---

### **Step 4: Generator evaluates relevance** `[ISREL]`

- Reads retrieved documents and decides which are relevant:

```
Doc A â†’ [ISREL=1] âœ… (directly relevant)
Doc B â†’ [ISREL=0] ğŸš« (irrelevant)

```

- Focuses only on relevant docs for generation.
---
### **Step 5: Generator writes output and predicts support** `[ISSUP]`

- For each generated statement, checks if it is supported by the relevant documents:

```
Sentence: â€œMorocco became the first African team to reach the semifinals in 2022.â€
[ISSUP=1] âœ… (fully supported by Doc A)

```

---

### **Step 6: Generator predicts usefulness** `[ISUSE]`

- Evaluates if the statement is useful for answering the query:

```
[ISUSE=5] â­â­â­â­â­ â†’ extremely useful, main answer to the query

```
---
### **Step 7: Optional second retrieval**

- If the generator finds evidence weak or missing, it can **retrieve again**:

```
[RETRIEVE=CONTINUE] ğŸ”„ â†’ reuse previous docs or fetch new ones

```

- In this case, Doc A already sufficient, no new retrieval needed.

---

### **Step 8: Final output**

- Generator combines all decisions and outputs the final answer **with annotations**:

> [RETRIEVE=YES] Morocco became the first African team to reach the semifinals in 2022. [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=5] â­â­â­â­â­
> 
- This output is **factually supported, relevant, and highly useful**. âœ…ğŸ’¡
---
### **Step 9: Critic model role (training phase)**

- During training, the Critic (C) would label retrieved documents and outputs with reflection tokens.
- Generator learns from this data to self-reflect during inference.

---

### **Step 10: Self-reflection & quality control**

- Generator can **reevaluate its own output** using reflection tokens:
    - Check relevance `[ISREL]`
    - Check support `[ISSUP]`
    - Check usefulness `[ISUSE]`
- Ensures **final answer is accurate and useful** before returning to the user. ğŸ†

---

## **ğŸ’¡ Key Takeaways**

âœ… **RAG** improves factual grounding by adding retrieval â€” but itâ€™s rigid.

âœ… **SELF-RAG** adds **self-awareness**: retrieves when needed, critiques its logic, and balances fluency vs factuality.

âœ… **Reflection tokens** turn reasoning into structured signals.

âœ… **Critic + Generator** training removes the need for complex reinforcement learning.

âœ… **End result:** Efficient, factual, transparent LLM reasoning.

<aside>
ğŸ’¡

SELF-RAG combines **adaptive retrieval** + **self-reflection** to produce **more factual, efficient, and transparent outputs** than standard RAG or even larger LLMs.

</aside>
