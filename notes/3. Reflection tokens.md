# SELF-RAG â€” Self-Reflective Retrieval-Augmented Generation â€” **Reflection Tokens in SELF-RAG**

**Reflection tokens** are **special markers** that help the model **analyze and monitor its own reasoning process** during generation.

They tell the model *how **confident, supported**, or **relevant*** its statements are.


## ğŸ§© **Main Reflection Tokens**

| Token | Meaning | Purpose |
| --- | --- | --- |
| `[RETRIEVE]` | Indicates if the model should fetch external information ğŸ” | Ensures answers are factually supported by relevant documents âœ… |
| `[ISREL]` | Is the **retrieved document relevant** ? ğŸ“š | Judges if the **retrieved doc** actually helps **answer the question.** |
| `[ISSUP]` | Is the statement **supported** by evidence ***( retrieved doc)***? âœ… | Marks whether the **generated answer**  matches **retrieved docs.** |
| `[ISUSE]` | How **useful** the document is for answering â­ | Measures doc usefulness (e.g., 1â€“5 scale). |
<img width="861" height="392" alt="image" src="https://github.com/user-attachments/assets/1939f7fd-a78e-4b45-ae3b-b9ea831e628b" />

## 1ï¸âƒ£ğŸ” **[RETRIEVE] â€” â€œShould I look something up?â€**

### ğŸ§  **Meaning**

The `[RETRIEVE]` token tells the model **when to search for external information** (from a knowledge base, documents, or database).

Itâ€™s like the model saying to itself:

> â€œHmmâ€¦ Iâ€™m not sure enough â€” I should check the facts before answering.â€
> 

---

### âš™ï¸ **When the model uses [RETRIEVE]**

- When the **question is factual**, and the model **isnâ€™t confident in its memory** â“
- When it needs **up-to-date or detailed info** that might **not** be in **its internal knowledge** ğŸ—‚ï¸
- It can also **skip retrieval** if itâ€™s **confident** â†’ `[RETRIEVE=NO]` ğŸš«

---

### âš½ **Example 1: Morocco in the World Cup**

### ğŸ² **Example 2: How to make couscous**

> User: â€œHow did Morocco perform in the 2022 FIFA World Cup?â€ ğŸ‡²ğŸ‡¦âš½
> 

**Model thinking:**

> â€œHmm, thatâ€™s a factual eventâ€¦ I might not remember the details.â€ ğŸ¤”
> 

So it triggers retrieval:

```
**[RETRIEVE]**
(retrieves FIFA articles ğŸ“„)
Finds: â€œMorocco reached the semifinals.â€
Generates: â€œMorocco became the first African team to reach the World Cup semifinals.â€

```

âœ… **Result:** Retrieval was necessary because the model needed ***real-world data*.**

> User: â€œHow do Moroccans traditionally prepare couscous?â€ ğŸ²ğŸ‡²ğŸ‡¦
> 

**Model thinking:**

> â€œI already know the general recipe â€” I donâ€™t need to look it up.â€ ğŸ˜
> 

So it skips retrieval:

```
**[RETRIEVE=NO]**
Generates: â€œMoroccan couscous is steamed semolina served with vegetables and meat, usually on Fridays.â€

```

âœ… **Result:** No retrieval needed â€” **the answer relies on stored cultural knowledge.**

---

### ğŸ’¡ **Summary**

| Mode | Token | Description | Example |
| --- | --- | --- | --- |
| Retrieve | `[RETRIEVE]` | Model decides to look for info ğŸ” | Morocco in World Cup âš½ |
| Skip | `[RETRIEVE=NO]` | Model confident, no search ğŸš« | Moroccan couscous ğŸ² |

### ğŸ§© **In short**

**[RETRIEVE]** = the modelâ€™s *curiosity switch* ğŸª„

It tells the system:

> â€œDo I know this, or should I check before I speak?â€ ğŸ§ ğŸ”
> 

---

## 2ï¸âƒ£âœ… **[ISREL] â€” â€œIs this document relevant?â€**

### ğŸ§  **Meaning**

The `[ISREL]` token helps the model **judge how relevant** a retrieved document or passage is to the userâ€™s question.

Itâ€™s like the model saying:

> â€œI found some infoâ€¦ but is it actually about what the user asked?â€ ğŸ¤”
> 

---

### âš™ï¸ **When the model uses [ISREL]**

- After **retrieving** several documents ğŸ“š
- It checks each one and decides:
    - `[ISREL=1]` âœ… â†’ The doc is **relevant** to the question
    - `[ISREL=0]` ğŸš« â†’ The doc is **irrelevant** or off-topic

This helps the model **focus only on useful evidence** for generating the final answer ğŸ’¡

---

### âš½ **Example 1: Morocco in the 2022 World Cup** ğŸ‡²ğŸ‡¦âš½

> User: â€œHow did Morocco perform in the 2022 World Cup?â€
> 

**Model retrieves two documents:**

1ï¸âƒ£ Doc A: â€œMorocco reached the semifinals after defeating Portugal.â€

2ï¸âƒ£ Doc B: â€œFrance won the 2018 World Cup.â€

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ… (directly about Morocco 2022)
Doc B â†’ [ISREL=0] ğŸš« (irrelevant year/event)
```

âœ… It then focuses on **Doc A** to generate:

> â€œMorocco became the first African team to reach the semifinals in 2022.â€ âš½ğŸŒ
> 

### ğŸœï¸ **Example 2: Moroccoâ€™s desert climate** â˜€ï¸ğŸœï¸

> User: â€œWhat kind of climate does Morocco have?â€
> 

**Model retrieves:**

1ï¸âƒ£ Doc A: â€œMorocco has a Mediterranean climate in the north and desert climate in the south.â€

2ï¸âƒ£ Doc B: â€œMoroccoâ€™s football team made history in 2022.â€

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ…
Doc B â†’ [ISREL=0] ğŸš«
```

âœ… Only **Doc A** is used to generate:

> â€œMorocco has a mix of Mediterranean and desert climates.â€ ğŸŒ¦ï¸ğŸŒµ
> 

---

### ğŸ’¡ **Summary**

| Token | Meaning | Example | Decision |
| --- | --- | --- | --- |
| `[ISREL=1]` | Doc is relevant to the query âœ… | Morocco in 2022 World Cup | Keep it |
| `[ISREL=0]` | Doc is irrelevant ğŸš« | France 2018 World Cup | Ignore it |

### ğŸ§© **In short**

**[ISREL]** = the modelâ€™s *focus filter* ğŸ¯

It helps the generator **ignore noise** and **keep only the right context**:

> â€œIs this really about what the user asked?â€ ğŸ§âœ…
> 

---

## 3ï¸âƒ£ ğŸ“– **[ISSUP] â€” â€œIs this statement supported by evidence?â€**

### ğŸ§  **Meaning**

The `[ISSUP]` token helps the model decide **whether what itâ€™s saying is truly backed by the retrieved documents** ğŸ§¾ğŸ”.

Itâ€™s like the model asking itself:

> â€œCan I prove what I just said with evidence ? (retrieved docs)â€ ğŸ¤”ğŸ“š
> 

---

### âš™ï¸ **When the model uses [ISSUP]**

- After **retrieving** and **checking relevance** (`[ISREL]`) âœ…
- The model reads the selected docs and compares them to its own generated statements ğŸ§©
- Then it assigns:
    - `[ISSUP=1]` âœ… â†’ statement is **supported** by evidence
    - `[ISSUP=0]` ğŸš« â†’ statement is **unsupported** or **contradicted**

This token teaches the model to **speak only when it has proof** ğŸ“Š

---

### âš½ **Example 1: Morocco in the 2022 World Cup** ğŸ‡²ğŸ‡¦âš½

> User: â€œDid Morocco reach the semifinals in the 2022 World Cup?â€ ğŸ†
> 

**Model reasoning:**

```
Iâ€™m not 100% sure ğŸ¤” â†’ [RETRIEVE=YES] ğŸ”
(retrieves FIFA 2022 documents ğŸ“‘)
```

**Model retrieves:**

- Doc A: â€œMorocco reached the semifinals after defeating Portugal.â€
- Doc B: â€œCroatia won the third-place match against Morocco.â€

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ…
Doc B â†’ [ISREL=1] âœ…

```

**Generated statement:**

> â€œMorocco reached the semifinals in the 2022 World Cup.â€ âš½ğŸ‡²ğŸ‡¦
> 

**Support check:**

```
Statement matches Doc A â†’ [ISSUP=1] âœ…
```

âœ… So the model marks it as supported and confident:

> â€œ[RETRIEVE] Morocco reached the semifinals in the 2022 World Cup. [ISSUP=1] âœ…â€
> 

---

### ğŸ”ï¸ **Example 2: Moroccoâ€™s mountains** â›°ï¸ğŸ‡²ğŸ‡¦

> User: â€œIs the tallest mountain in Morocco Mount Everest?â€ ğŸ˜…
> 

**Model reasoning:**

```
That sounds suspicious... ğŸ¤¨ â†’ [RETRIEVE=YES] ğŸ”
(retrieves geography docs ğŸ“„)
```

**Model retrieves:**

- Doc A: â€œToubkal is the highest peak in Morocco and North Africa.â€
- Doc B: â€œMount Everest is located in Nepal.â€

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ…
Doc B â†’ [ISREL=1] âœ…
```

**Generated statement:**

> â€œThe tallest mountain in Morocco is Mount Everest.â€ ğŸ”ï¸
> 

**Support check:**

```
No document supports this âŒ â†’ [ISSUP=0] ğŸš«
```

ğŸš« The model realizes the claim is **not supported**, so it **revises** the answer:

> â€œThe tallest mountain in Morocco is Mount Toubkal.â€ â›°ï¸ğŸ‡²ğŸ‡¦ â†’ [ISSUP=1] âœ…
> 

---

### ğŸ’¡ **Summary**

| Token | Meaning | Example | Decision |
| --- | --- | --- | --- |
| `[ISSUP=1]` | Statement is supported by evidence âœ… | Morocco reached semifinals | Keep it |
| `[ISSUP=0]` | Statement not supported ğŸš« | Everest in Morocco | Correct it |

---

### ğŸ§© **In short**

**[ISSUP]** = the modelâ€™s *truth detector* ğŸ•µï¸â€â™‚ï¸ğŸ“–

It ensures every generated fact has **evidence** behind it.

> â€œAm I saying something thatâ€™s actually in the documents?â€ ğŸ§âœ…
> 

---

Hereâ€™s a full and visual explanation of the **[ISUSE]** token â€” with **2 detailed examples** and **emojis** ğŸ‘‡

---

## 4ï¸âƒ£â­ **[ISUSE] â€” â€œHow useful is this document or sentence for answering the question?â€**

### ğŸ§  **Meaning**

The `[ISUSE]` token measures **how valuable** a piece of information (a document, passage, or sentence) is for producing the *best possible answer* ğŸ†.

Itâ€™s like the model asking itself:

> â€œOkay, this info is relevantâ€¦ but is it actually helpful for answering the user?â€ ğŸ¤”ğŸ’­
> 

---

### âš™ï¸ **When the model uses [ISUSE]**

- After checking **relevance** (`[ISREL]`) and **support** (`[ISSUP]`) âœ…ğŸ“–
- It assigns a **usefulness score** (often from 1 to 5 â­) to show how *strongly* a piece of text contributes to the final answer:

| Token |  |
| --- | --- |
| `[ISUSE=5]` â­â­â­â­â­ â†’ Extremely useful |  |
| `[ISUSE=3]` â­â­â­ â†’ Moderately useful |  |
| `[ISUSE=1]` â­ â†’ Barely useful |  |

This helps the model **prioritize** the most informative parts when generating its response ğŸ’¬âš™ï¸

---

### âš½ **Example 1: Morocco in the 2022 World Cup** ğŸ‡²ğŸ‡¦âš½

> User: â€œWhat was Moroccoâ€™s biggest achievement in the 2022 World Cup?â€ ğŸ†ğŸ‡²ğŸ‡¦
> 

**Model reasoning:**

```
Iâ€™ll need details about Moroccoâ€™s results â†’ [RETRIEVE=YES] ğŸ”
```

**Model retrieves:**

1ï¸âƒ£ Doc A: â€œMorocco became the first African and Arab team to reach the semifinals.â€

2ï¸âƒ£ Doc B: â€œMoroccoâ€™s coach Walid Regragui took charge in August 2022.â€

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=5] â­â­â­â­â­
Doc B â†’ [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=2] â­â­ (less useful for the question)

```

âœ… **Final generation:**

> â€œ[RETRIEVE] Morocco became the first African and Arab team to reach the semifinals â€” its greatest achievement in 2022. [ISUSE=5] â­â­â­â­â­â€
> 

ğŸ’¡ The model *kept* Doc A because it was **the most useful** for the specific question.

---

### ğŸœï¸ **Example 2: Moroccoâ€™s desert climate** â˜€ï¸ğŸœï¸

> User: â€œDescribe Moroccoâ€™s main climate zones.â€ ğŸŒ¦ï¸ğŸŒµ
> 

**Model reasoning:**

```
Letâ€™s fetch info about Moroccan geography â†’ [RETRIEVE=YES] ğŸ”
```

**Model retrieves:**

1ï¸âƒ£ Doc A: â€œMorocco has a Mediterranean climate in the north and a desert climate in the south.â€

2ï¸âƒ£ Doc B: â€œMoroccoâ€™s national dish is couscous.â€ ğŸ²

**Model evaluation:**

```
Doc A â†’ [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=5] â­â­â­â­â­ (directly useful)
Doc B â†’ [ISREL=0] ğŸš« [ISSUP=0] ğŸš« [ISUSE=1] â­ (off-topic)

```

âœ… **Final generation:**

> â€œ[RETRIEVE] Morocco has both Mediterranean and desert climates, creating diverse weather conditions. [ISUSE=5] â­â­â­â­â­â€
> 

ğŸš« The model **ignored Doc B** because it was **irrelevant and not useful** for describing climate.

---

### ğŸ’¡ **Summary**

| Token | Meaning | Example | Action |
| --- | --- | --- | --- |
| `[ISUSE=5]` â­â­â­â­â­ | Extremely useful info | Morocco reached semifinals | Keep |
| `[ISUSE=2]` â­â­ | Minor detail | Coach name | Optional |
| `[ISUSE=1]` â­ | Off-topic | Moroccan food | Ignore |

---


### ğŸ§© **In short**

**[ISUSE]** = the modelâ€™s *priority meter* ğŸ¯â­

It helps the model choose **the most helpful evidence** when forming answers â€” filtering whatâ€™s **essential** vs. **extra**.

> â€œThis info is trueâ€¦ but is it actually useful for the userâ€™s question?â€ ğŸ§ ğŸ’¬â­
>

<img width="8948" height="4734" alt="teaser_self_rag_v8" src="https://github.com/user-attachments/assets/eeb94b85-d6d7-4dec-b9bc-15e952c9ea54" />
