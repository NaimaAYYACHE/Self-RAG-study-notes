# SELF-RAG ‚Äî Self-Reflective Retrieval-Augmented Generation ‚Äî **Reflection Tokens in SELF-RAG**

**Reflection tokens** are **special markers** that help the model **analyze and monitor its own reasoning process** during generation.

They tell the model *how **confident, supported**, or **relevant*** its statements are.


## üß© **Main Reflection Tokens**

| Token | Meaning | Purpose |
| --- | --- | --- |
| `[RETRIEVE]` | Indicates if the model should fetch external information üîç | Ensures answers are factually supported by relevant documents ‚úÖ |
| `[ISREL]` | Is the **retrieved document relevant** ? üìö | Judges if the **retrieved doc** actually helps **answer the question.** |
| `[ISSUP]` | Is the statement **supported** by evidence ***( retrieved doc)***? ‚úÖ | Marks whether the **generated answer**  matches **retrieved docs.** |
| `[ISUSE]` | How **useful** the document is for answering ‚≠ê | Measures doc usefulness (e.g., 1‚Äì5 scale). |

## üîç **[RETRIEVE] ‚Äî ‚ÄúShould I look something up?‚Äù**

### üß† **Meaning**

The `[RETRIEVE]` token tells the model **when to search for external information** (from a knowledge base, documents, or database).

It‚Äôs like the model saying to itself:

> ‚ÄúHmm‚Ä¶ I‚Äôm not sure enough ‚Äî I should check the facts before answering.‚Äù
> 

---

### ‚öôÔ∏è **When the model uses [RETRIEVE]**

- When the **question is factual**, and the model **isn‚Äôt confident in its memory** ‚ùì
- When it needs **up-to-date or detailed info** that might **not** be in **its internal knowledge** üóÇÔ∏è
- It can also **skip retrieval** if it‚Äôs **confident** ‚Üí `[RETRIEVE=NO]` üö´

---

### ‚öΩ **Example 1: Morocco in the World Cup**

### üç≤ **Example 2: How to make couscous**

> User: ‚ÄúHow did Morocco perform in the 2022 FIFA World Cup?‚Äù üá≤üá¶‚öΩ
> 

**Model thinking:**

> ‚ÄúHmm, that‚Äôs a factual event‚Ä¶ I might not remember the details.‚Äù ü§î
> 

So it triggers retrieval:

```
**[RETRIEVE]**
(retrieves FIFA articles üìÑ)
Finds: ‚ÄúMorocco reached the semifinals.‚Äù
Generates: ‚ÄúMorocco became the first African team to reach the World Cup semifinals.‚Äù

```

‚úÖ **Result:** Retrieval was necessary because the model needed ***real-world data*.**

> User: ‚ÄúHow do Moroccans traditionally prepare couscous?‚Äù üç≤üá≤üá¶
> 

**Model thinking:**

> ‚ÄúI already know the general recipe ‚Äî I don‚Äôt need to look it up.‚Äù üòé
> 

So it skips retrieval:

```
**[RETRIEVE=NO]**
Generates: ‚ÄúMoroccan couscous is steamed semolina served with vegetables and meat, usually on Fridays.‚Äù

```

‚úÖ **Result:** No retrieval needed ‚Äî **the answer relies on stored cultural knowledge.**

---

### üí° **Summary**

| Mode | Token | Description | Example |
| --- | --- | --- | --- |
| Retrieve | `[RETRIEVE]` | Model decides to look for info üîç | Morocco in World Cup ‚öΩ |
| Skip | `[RETRIEVE=NO]` | Model confident, no search üö´ | Moroccan couscous üç≤ |

### üß© **In short**

**[RETRIEVE]** = the model‚Äôs *curiosity switch* ü™Ñ

It tells the system:

> ‚ÄúDo I know this, or should I check before I speak?‚Äù üß†üîç
> 

---

## ‚úÖ **[ISREL] ‚Äî ‚ÄúIs this document relevant?‚Äù**

### üß† **Meaning**

The `[ISREL]` token helps the model **judge how relevant** a retrieved document or passage is to the user‚Äôs question.

It‚Äôs like the model saying:

> ‚ÄúI found some info‚Ä¶ but is it actually about what the user asked?‚Äù ü§î
> 

---

### ‚öôÔ∏è **When the model uses [ISREL]**

- After **retrieving** several documents üìö
- It checks each one and decides:
    - `[ISREL=1]` ‚úÖ ‚Üí The doc is **relevant** to the question
    - `[ISREL=0]` üö´ ‚Üí The doc is **irrelevant** or off-topic

This helps the model **focus only on useful evidence** for generating the final answer üí°

---

### ‚öΩ **Example 1: Morocco in the 2022 World Cup** üá≤üá¶‚öΩ

> User: ‚ÄúHow did Morocco perform in the 2022 World Cup?‚Äù
> 

**Model retrieves two documents:**

1Ô∏è‚É£ Doc A: ‚ÄúMorocco reached the semifinals after defeating Portugal.‚Äù

2Ô∏è‚É£ Doc B: ‚ÄúFrance won the 2018 World Cup.‚Äù

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ (directly about Morocco 2022)
Doc B ‚Üí [ISREL=0] üö´ (irrelevant year/event)
```

‚úÖ It then focuses on **Doc A** to generate:

> ‚ÄúMorocco became the first African team to reach the semifinals in 2022.‚Äù ‚öΩüåç
> 

### üèúÔ∏è **Example 2: Morocco‚Äôs desert climate** ‚òÄÔ∏èüèúÔ∏è

> User: ‚ÄúWhat kind of climate does Morocco have?‚Äù
> 

**Model retrieves:**

1Ô∏è‚É£ Doc A: ‚ÄúMorocco has a Mediterranean climate in the north and desert climate in the south.‚Äù

2Ô∏è‚É£ Doc B: ‚ÄúMorocco‚Äôs football team made history in 2022.‚Äù

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ
Doc B ‚Üí [ISREL=0] üö´
```

‚úÖ Only **Doc A** is used to generate:

> ‚ÄúMorocco has a mix of Mediterranean and desert climates.‚Äù üå¶Ô∏èüåµ
> 

---

### üí° **Summary**

| Token | Meaning | Example | Decision |
| --- | --- | --- | --- |
| `[ISREL=1]` | Doc is relevant to the query ‚úÖ | Morocco in 2022 World Cup | Keep it |
| `[ISREL=0]` | Doc is irrelevant üö´ | France 2018 World Cup | Ignore it |

### üß© **In short**

**[ISREL]** = the model‚Äôs *focus filter* üéØ

It helps the generator **ignore noise** and **keep only the right context**:

> ‚ÄúIs this really about what the user asked?‚Äù üßê‚úÖ
> 

---

## üìñ **[ISSUP] ‚Äî ‚ÄúIs this statement supported by evidence?‚Äù**

### üß† **Meaning**

The `[ISSUP]` token helps the model decide **whether what it‚Äôs saying is truly backed by the retrieved documents** üßæüîç.

It‚Äôs like the model asking itself:

> ‚ÄúCan I prove what I just said with evidence ? (retrieved docs)‚Äù ü§îüìö
> 

---

### ‚öôÔ∏è **When the model uses [ISSUP]**

- After **retrieving** and **checking relevance** (`[ISREL]`) ‚úÖ
- The model reads the selected docs and compares them to its own generated statements üß©
- Then it assigns:
    - `[ISSUP=1]` ‚úÖ ‚Üí statement is **supported** by evidence
    - `[ISSUP=0]` üö´ ‚Üí statement is **unsupported** or **contradicted**

This token teaches the model to **speak only when it has proof** üìä

---

### ‚öΩ **Example 1: Morocco in the 2022 World Cup** üá≤üá¶‚öΩ

> User: ‚ÄúDid Morocco reach the semifinals in the 2022 World Cup?‚Äù üèÜ
> 

**Model reasoning:**

```
I‚Äôm not 100% sure ü§î ‚Üí [RETRIEVE=YES] üîç
(retrieves FIFA 2022 documents üìë)
```

**Model retrieves:**

- Doc A: ‚ÄúMorocco reached the semifinals after defeating Portugal.‚Äù
- Doc B: ‚ÄúCroatia won the third-place match against Morocco.‚Äù

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ
Doc B ‚Üí [ISREL=1] ‚úÖ

```

**Generated statement:**

> ‚ÄúMorocco reached the semifinals in the 2022 World Cup.‚Äù ‚öΩüá≤üá¶
> 

**Support check:**

```
Statement matches Doc A ‚Üí [ISSUP=1] ‚úÖ
```

‚úÖ So the model marks it as supported and confident:

> ‚Äú[RETRIEVE] Morocco reached the semifinals in the 2022 World Cup. [ISSUP=1] ‚úÖ‚Äù
> 

---

### üèîÔ∏è **Example 2: Morocco‚Äôs mountains** ‚õ∞Ô∏èüá≤üá¶

> User: ‚ÄúIs the tallest mountain in Morocco Mount Everest?‚Äù üòÖ
> 

**Model reasoning:**

```
That sounds suspicious... ü§® ‚Üí [RETRIEVE=YES] üîç
(retrieves geography docs üìÑ)
```

**Model retrieves:**

- Doc A: ‚ÄúToubkal is the highest peak in Morocco and North Africa.‚Äù
- Doc B: ‚ÄúMount Everest is located in Nepal.‚Äù

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ
Doc B ‚Üí [ISREL=1] ‚úÖ
```

**Generated statement:**

> ‚ÄúThe tallest mountain in Morocco is Mount Everest.‚Äù üèîÔ∏è
> 

**Support check:**

```
No document supports this ‚ùå ‚Üí [ISSUP=0] üö´
```

üö´ The model realizes the claim is **not supported**, so it **revises** the answer:

> ‚ÄúThe tallest mountain in Morocco is Mount Toubkal.‚Äù ‚õ∞Ô∏èüá≤üá¶ ‚Üí [ISSUP=1] ‚úÖ
> 

---

### üí° **Summary**

| Token | Meaning | Example | Decision |
| --- | --- | --- | --- |
| `[ISSUP=1]` | Statement is supported by evidence ‚úÖ | Morocco reached semifinals | Keep it |
| `[ISSUP=0]` | Statement not supported üö´ | Everest in Morocco | Correct it |

---

### üß© **In short**

**[ISSUP]** = the model‚Äôs *truth detector* üïµÔ∏è‚Äç‚ôÇÔ∏èüìñ

It ensures every generated fact has **evidence** behind it.

> ‚ÄúAm I saying something that‚Äôs actually in the documents?‚Äù üßê‚úÖ
> 

---

Here‚Äôs a full and visual explanation of the **[ISUSE]** token ‚Äî with **2 detailed examples** and **emojis** üëá

---

## ‚≠ê **[ISUSE] ‚Äî ‚ÄúHow useful is this document or sentence for answering the question?‚Äù**

### üß† **Meaning**

The `[ISUSE]` token measures **how valuable** a piece of information (a document, passage, or sentence) is for producing the *best possible answer* üèÜ.

It‚Äôs like the model asking itself:

> ‚ÄúOkay, this info is relevant‚Ä¶ but is it actually helpful for answering the user?‚Äù ü§îüí≠
> 

---

### ‚öôÔ∏è **When the model uses [ISUSE]**

- After checking **relevance** (`[ISREL]`) and **support** (`[ISSUP]`) ‚úÖüìñ
- It assigns a **usefulness score** (often from 1 to 5 ‚≠ê) to show how *strongly* a piece of text contributes to the final answer:

| Token |  |
| --- | --- |
| `[ISUSE=5]` ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚Üí Extremely useful |  |
| `[ISUSE=3]` ‚≠ê‚≠ê‚≠ê ‚Üí Moderately useful |  |
| `[ISUSE=1]` ‚≠ê ‚Üí Barely useful |  |

This helps the model **prioritize** the most informative parts when generating its response üí¨‚öôÔ∏è

---

### ‚öΩ **Example 1: Morocco in the 2022 World Cup** üá≤üá¶‚öΩ

> User: ‚ÄúWhat was Morocco‚Äôs biggest achievement in the 2022 World Cup?‚Äù üèÜüá≤üá¶
> 

**Model reasoning:**

```
I‚Äôll need details about Morocco‚Äôs results ‚Üí [RETRIEVE=YES] üîç
```

**Model retrieves:**

1Ô∏è‚É£ Doc A: ‚ÄúMorocco became the first African and Arab team to reach the semifinals.‚Äù

2Ô∏è‚É£ Doc B: ‚ÄúMorocco‚Äôs coach Walid Regragui took charge in August 2022.‚Äù

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ [ISSUP=1] ‚úÖ [ISUSE=5] ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Doc B ‚Üí [ISREL=1] ‚úÖ [ISSUP=1] ‚úÖ [ISUSE=2] ‚≠ê‚≠ê (less useful for the question)

```

‚úÖ **Final generation:**

> ‚Äú[RETRIEVE] Morocco became the first African and Arab team to reach the semifinals ‚Äî its greatest achievement in 2022. [ISUSE=5] ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚Äù
> 

üí° The model *kept* Doc A because it was **the most useful** for the specific question.

---

### üèúÔ∏è **Example 2: Morocco‚Äôs desert climate** ‚òÄÔ∏èüèúÔ∏è

> User: ‚ÄúDescribe Morocco‚Äôs main climate zones.‚Äù üå¶Ô∏èüåµ
> 

**Model reasoning:**

```
Let‚Äôs fetch info about Moroccan geography ‚Üí [RETRIEVE=YES] üîç
```

**Model retrieves:**

1Ô∏è‚É£ Doc A: ‚ÄúMorocco has a Mediterranean climate in the north and a desert climate in the south.‚Äù

2Ô∏è‚É£ Doc B: ‚ÄúMorocco‚Äôs national dish is couscous.‚Äù üç≤

**Model evaluation:**

```
Doc A ‚Üí [ISREL=1] ‚úÖ [ISSUP=1] ‚úÖ [ISUSE=5] ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (directly useful)
Doc B ‚Üí [ISREL=0] üö´ [ISSUP=0] üö´ [ISUSE=1] ‚≠ê (off-topic)

```

‚úÖ **Final generation:**

> ‚Äú[RETRIEVE] Morocco has both Mediterranean and desert climates, creating diverse weather conditions. [ISUSE=5] ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚Äù
> 

üö´ The model **ignored Doc B** because it was **irrelevant and not useful** for describing climate.

---

### üí° **Summary**

| Token | Meaning | Example | Action |
| --- | --- | --- | --- |
| `[ISUSE=5]` ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Extremely useful info | Morocco reached semifinals | Keep |
| `[ISUSE=2]` ‚≠ê‚≠ê | Minor detail | Coach name | Optional |
| `[ISUSE=1]` ‚≠ê | Off-topic | Moroccan food | Ignore |

---
<img width="8948" height="4734" alt="teaser_self_rag_v8" src="https://github.com/user-attachments/assets/eeb94b85-d6d7-4dec-b9bc-15e952c9ea54" />

### üß© **In short**

**[ISUSE]** = the model‚Äôs *priority meter* üéØ‚≠ê

It helps the model choose **the most helpful evidence** when forming answers ‚Äî filtering what‚Äôs **essential** vs. **extra**.

> ‚ÄúThis info is true‚Ä¶ but is it actually useful for the user‚Äôs question?‚Äù üß†üí¨‚≠ê
>
