## **SELF-RAG: Example â€” Moroccoâ€™s 2022 World Cup Achievement** ğŸ†ğŸ‡²ğŸ‡¦

### **Step 1: User Query**

> User: â€œWhat was Moroccoâ€™s biggest achievement in the 2022 World Cup?â€ âš½ğŸ‡²ğŸ‡¦

The system receives the query and the **Generator (M)** starts reasoning:

> ğŸ’­ â€œDo I have enough info?â€
> ğŸ¤” â€œHmm, thatâ€™s a factual eventâ€¦ I need details about the 2022 World Cup.â€

---

### **Step 2: Generator decides whether to retrieve** `[RETRIEVE]`

* The generator checks its internal knowledge:

  * If confident â†’ `[RETRIEVE=NO]` âœ…
  * If unsure â†’ `[RETRIEVE=YES]` ğŸ” *(triggers retrieval)*

**In this case:**

```
[RETRIEVE=YES] ğŸ” (not stored in model memory)
```

---

### **Step 3: Retriever fetches documents**

* The **Adaptive Retriever** searches the knowledge base.
* It returns multiple documents related to Moroccoâ€™s 2022 World Cup.

**Retrieved docs:**

ğŸ“„ **Doc A:** â€œMorocco became the first African and Arab team to reach the semifinals.â€
ğŸ“„ **Doc B:** â€œMoroccoâ€™s coach Walid Regragui took charge in August 2022.â€
ğŸ“„ **Doc C:** â€œArgentina won the 2022 World Cup.â€

---

### **Step 4: Generator evaluates relevance** `[ISREL]`

* The generator reads all three documents and marks relevance:

```
Doc A â†’ [ISREL=1] âœ… (directly relevant to Moroccoâ€™s achievement)
Doc B â†’ [ISREL=1] âœ… (contextually relevant)
Doc C â†’ [ISREL=0] ğŸš« (irrelevant to Morocco)
```

âœ… Focuses only on **Doc A** and **Doc B** for generating the answer.

---

### **Step 5: Generator writes outputs and predicts support** `[ISSUP]`

* The generator now creates factual sentences based on the relevant docs.

```
ğŸ† From Doc A â†’ â€œMorocco was the first African and Arab team to reach the World Cup semifinals.â€
ğŸ‘” From Doc B â†’ â€œWalid Regragui became Moroccoâ€™s coach in August 2022.â€
```

* It checks if both statements are **supported** by the documents:

```
Doc A sentence â†’ [ISSUP=1] âœ…
Doc B sentence â†’ [ISSUP=1] âœ…
```

---

### **Step 6: Generator predicts usefulness** `[ISUSE]`

* The generator evaluates which answer best addresses the userâ€™s question:

```
ğŸ† Doc A â†’ [ISUSE=5] â­â­â­â­â­ â†’ extremely useful (main achievement)
ğŸ‘” Doc B â†’ [ISUSE=3] â­â­â­ â†’ moderately useful (background info)
```

---

### **Step 7: Optional second retrieval**

* If the model finds missing evidence, it may **retrieve again**:

```
[RETRIEVE=CONTINUE] ğŸ”„ â†’ fetch new docs if needed
```

* In this case, **Doc A** and **Doc B** are enough â€” no second retrieval needed.

---

### **Step 8: Final output**

The model combines its decisions and produces the **final factual answer**:

> [RETRIEVE=YES] Morocco was the first African and Arab team to reach the World Cup semifinals. [ISREL=1] âœ… [ISSUP=1] âœ… [ISUSE=5] â­â­â­â­â­

âœ… The answer is **relevant**, **supported**, and **highly useful**.

---

### **Step 9: Critic model role (training phase)**

* During training, the **Critic (C)** labels retrieved docs and model outputs with reflection tokens.
* The **Generator (M)** learns to self-assess its relevance, support, and usefulness at inference time.

---

### **Step 10: Self-reflection & quality control**

* The generator can **reinspect** its reasoning path using reflection tokens:

  * Check relevance `[ISREL]`
  * Check support `[ISSUP]`
  * Check usefulness `[ISUSE]`

âœ… This ensures the **final answer is grounded, clear, and trustworthy** before returning to the user.

---

## **ğŸ’¡ Key Takeaways**

âœ… **SELF-RAG** integrates **retrieval + reflection** â€” the model learns *when* and *how* to fetch external data.

âœ… It performs **relevance**, **support**, and **usefulness** evaluations for transparent reasoning.

âœ… The **Critic** trains the generator to self-improve â€” no reinforcement learning required.

âœ… Results: **Adaptive, factual, and interpretable** responses.

---

ğŸ’¡ SELF-RAG combines **adaptive retrieval** ğŸ” + **self-reflection** ğŸª to create LLMs that are **factually grounded**, **efficient**, and **self-aware** â€” outperforming traditional RAG or larger non-reflective models.

![Design sans titre](https://github.com/user-attachments/assets/e589fa88-caec-46c8-8776-61fb3f05dde8)
