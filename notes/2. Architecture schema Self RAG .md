# **âš™ï¸ SELF-RAG â€” Self-Reflective Retrieval-Augmented Generation â€” Architecture Overview**

## 1ï¸âƒ£ Two Models

- **Generator Model (M)** âœï¸ğŸ¤– â†’ creates text and emits *reflection tokens* during generation
- **Critic Model (C)** ğŸ‘©â€ğŸ« â†’ evaluates and labels data with reflection tokens during training

> Together, they learn to retrieve, generate, and critique without external supervision.
> 
## Schema  :

<img width="8948" height="4734" alt="teaser_self_rag_v8" src="https://github.com/user-attachments/assets/dd007761-e699-40b1-b2fe-e0fc11b510ec" />

---

## ğŸ§© **Generator Model (M) â€” â€œThe Writer & Self-Reflectorâ€ âœï¸ğŸ¤–ğŸ§ **

### **Role**

- Its main job is to **generate text** in response to a user query ğŸ“ğŸ’¬.
- While generating, it **predicts reflection tokens** `([RETRIEVE] ğŸ”, [ISREL] âœ…, [ISSUP] ğŸ“–, [ISUSE] â­`) to guide its own reasoning ğŸ§ .
- It decides **when to retrieve information** ğŸ“š and whether the generated output is supported by evidence ğŸ“Š.
- Essentially, the Generator is the **decision maker & writer** âœï¸ğŸ§ , producing answers and self-critiquing them for quality and factuality ğŸ†.

---

### **When it works**

- **During inference** ğŸ”„ (i.e., when answering a user query).
    - **For every new user query**, the Generator:
        1. Decides if it needs to retrieve documents `([RETRIEVE]).`
        2. Evaluates relevance `([ISREL])` and support `([ISSUP]`) of retrieved info.
        3. Generates the answer while self-reflecting using these tokens `([ISUSE]).`
- **During training** ğŸ«, it learns to predict both **text** and **reflection tokens** from data annotated by the Critic model  ğŸ‘©â€ğŸ«.
    - The Generator **applies the Criticâ€™s lessons in real time** but without the ~~Critic mode~~l being involved.

---

### **Step-by-Step Generator Process**

**1ï¸âƒ£ Generator decides retrieval ğŸ”**

- The Generator evaluates its own knowledge when it starts generating.
    - Confident â†’ skip retrieval â†’ `[RETRIEVE=NO]` âœ…
    - Uncertain â†’ trigger retrieval â†’ `[RETRIEVE=YES]` ğŸ”
    - Reuse previous evidence â†’ `[RETRIEVE=CONTINUE]` ğŸ”„

**2ï¸âƒ£ Generator evaluates relevance of documents ğŸ“„**

- After retrieval, it reads retrieved passages.
- Judges each passageâ€™s relevance internally and assigns reflection tokens:
    - `[ISREL=1]` â†’ relevant âœ…
    - `[ISREL=0]` â†’ irrelevant âŒ
- Focuses only on **useful evidence** for the answer.

**3ï¸âƒ£ Generator produces reflection tokens alongside output ğŸ§ **

- Generates text **and reflection tokens together**:
    - `[ISREL]` â†’ relevance of retrieved doc
    - `[ISSUP]` â†’ whether generated statement is supported ğŸ“–
    - `[ISUSE]` â†’ usefulness of the sentence â­
- These tokens **guide reasoning step by step** and allow **self-reflection**.

**4ï¸âƒ£ Generator improves final output âœ¨**

- Using reflection tokens, it can:
    - **Refine answers** if evidence is weak ğŸ”„
    - **Select the best candidate** from multiple segments âœ…
    - **Balance fluency vs factuality** depending on the task ğŸ›ï¸

---

### **Example (Morocco 2022 âš½ğŸ‡²ğŸ‡¦)**

> User: â€œHow did Morocco perform in the 2022 World Cup?â€ âš½ğŸ‡²ğŸ‡¦
> 

Generatorâ€™s thinking process:

```
Hmmâ€¦ unsure ğŸ¤” â†’ [RETRIEVE] ğŸ”
(retrieves FIFA 2022 documents ğŸ“„ğŸ“‘)
Sentence: â€œMorocco became the first African and Arab team to reach the semifinals.â€
[ISREL=1] âœ… [ISSUP=1] ğŸ“– [ISUSE=5] â­

```

- The generator wrote the sentence **and** assigned reflection tokens ğŸ“ğŸ§ .
- These tokens **indicate what the model thinks about relevance, support, and usefulness** ğŸ”ğŸ“Š.
- This annotated output helps **improve generation quality** and **control inference behavior** ğŸ¯

---

## ğŸ§© **Critic Model (C) â€” â€œThe Teacher & Annotatorâ€ ğŸ‘©â€ğŸ«ğŸ“š**

### **Role**

- Its main job is **not to generate text** âŒğŸ“.
- Instead, it **evaluates outputs** (generated by the Generator or from a dataset) and **labels them with reflection tokens** ğŸ·ï¸.
- Essentially, it **provides â€œground truthâ€ reflection tokens** that the Generator can learn from ğŸ“–ğŸ§ .

---

### **When it works - Training Phase Only ğŸ‘©â€ğŸ«**

- The Critic model is used **during the training of the Generator** ğŸ«ğŸ“š..
- It **labels the training data** with reflection tokens:
    - Does the model need to retrieve external information? â†’ `[RETRIEVE] ğŸ”`
    - Are retrieved docs relevant? â†’ `[ISREL] âœ…`
    - Is the output supported by the documents? â†’ `[ISSUP] ğŸ“–`
    - Is the output useful to answer the query? â†’ `[ISUSE] â­`
- This teaches the Generator **how to self-reflect** when generating answers.
- âœ… After this training is done, the Criticâ€™s job is essentially complete. Itâ€™s **offline**.

---

### **Example (Morocco 2022 âš½ğŸ‡²ğŸ‡¦)**

Generator output:

> â€œMorocco became the first African and Arab team to reach the World Cup semifinals.â€
> 

Critic evaluates:

It annotates the training data like:

```
âœ… Relevant: doc discusses Morocco 2022 semifinals â†’ [ISREL=1] âœ…
âœ… Supported: statement matches doc â†’ [ISSUP=1] ğŸ“–
â­ Useful doc â†’ [ISUSE=5] â­

```

```
[RETRIEVE] Morocco became the first African and Arab team to reach 
the World Cup semifinals. [ISREL=1] âœ… [ISSUP=1] ğŸ“– [ISUSE=5] â­

```

- Generator then learns:

> â€œWhen I write similar sentences in the future, I should mark them as supported and relevant.â€ ğŸ§ ğŸ’¡
> 

---

## ğŸ”„ How Critic and Generator Interact

### **Critic Model (C) â€” the Teacher ğŸ‘©â€ğŸ«ğŸ“š**

- ğŸ‘€ Looks at the **retrieved documents** ğŸ“„ğŸ“„ and the **generatorâ€™s output** âœï¸ğŸ¤–.
- ğŸ·ï¸ Assigns **reflection tokens** `([RETRIEVE] ğŸ”, [ISREL] âœ…, [ISSUP] ğŸ“–, [ISUSE] â­)` to the training data.
- ğŸ“ These tokens **label :  which passages are relevant ?**, **which statements are supported ?** , and **how useful the answer is ?**  ğŸ“Š.
- âŒ Does **not generate answers** â€” it only **teaches the generator where and how to reflect** ğŸ§ .

### **Generator Model (M) â€” the Student âœï¸ğŸ¤–ğŸ§ **

- ğŸ“– Learns from the **Critic-labeled data** where reflection tokens should appear ? .
- ğŸ”„ During **inference**, it **generates answers** and **self-reflects** using these tokens.
- ğŸ•µï¸â€â™‚ï¸ Decides **when to retrieve ?** , **which docs are relevant ?** ğŸ“„, and **how well the answer is supported ?** ğŸ“Š.
- âœ¨ Can **refine its output** using reflection tokens, essentially doing **self-critique** ğŸ§.
  


## âš–ï¸ **Generator vs Critic â€“ Quick Comparison Table**

| Feature | Generator (M) âœï¸ğŸ¤– | Critic (C) ğŸ‘©â€ğŸ«ğŸ“š |
| --- | --- | --- |
| **Main job** | Generate text + predict reflection tokens ğŸ“ğŸ§  | Evaluate outputs + assign reflection tokens ğŸ·ï¸ğŸ“Š |
| **Active when** | Training & inference ğŸ”„ | Only during training ğŸ« |
| **User-facing** | Yes ğŸ’¬ | No âŒ |
| **Purpose** | Self-reflection & adaptive retrieval ğŸ”ğŸ§  | Provide ground truth labels for training ğŸ“– |
| **Example output** | Text + `[ISREL] [ISSUP] [ISUSE]` ğŸ“âœ…ğŸ“–â­ | Only `[ISREL] [ISSUP] [ISUSE]` labels ğŸ·ï¸âœ…ğŸ“–â­ |
| **Key actions** | Decide retrieval ğŸ•µï¸â€â™‚ï¸, judge relevance ğŸ“„, refine output ğŸ† | Label relevance âœ…, support ğŸ“–, usefulness â­ |

---
# **ğŸ“  How SELF-RAG Learns**

SELF-RAG training happens in **two main phases:**

## **1ï¸âƒ£ Offline Annotation (Critic Model C)**

- Uses a powerful LLM (like GPT-4) to insert reflection tokens into training data.
- The Critic model learns to predict those tokens automatically.

## **2ï¸âƒ£ Joint Training (Generator Model M)**

- The Generator learns both *what to write* and *when to reflect*.
- No need for reinforcement learning or reward models like RLHF â€” cheaper and more stable ğŸ’°.
---



### **Summary**

- **Critic:** Offline teacher during training only ğŸ“šğŸ‘©â€ğŸ«
- **Generator:** Active student during training and real-time thinker during inference âœï¸ğŸ¤–ğŸ§ 

